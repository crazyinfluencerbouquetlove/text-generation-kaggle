{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# LSTM layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  7900\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "# Selecting Edgar Allen Poe as author style to emulate\n",
    "author = train_df[train_df['author'] == 'EAP'][\"text\"]\n",
    "print('Number of sentences: ',author.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize words in corpus using Keras Tokenizer.\n",
    "This assigns a unique integer to each word and then replaces all instances of that word with the integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19, 2397, 80, 1001, 29, 31, 177, 2, 4073, 1, 1960, 2, 11, 3024, 15, 7, 110, 157, 41, 2146, 3, 481, 4, 1, 149, 2147, 7, 393, 74, 114, 101, 439, 2, 1, 162, 32, 913, 6453, 136, 1, 380], [6, 21, 142, 150, 10, 5, 551, 2148, 319, 28, 16, 15, 20, 8999, 128, 1, 3025, 2398, 30, 171, 2, 1797, 697, 20, 180, 2148, 6454, 12, 33, 188, 2, 1, 869, 243, 522, 1264], [1, 6455, 203, 14, 19, 149, 180, 6456, 6, 1, 1357, 2, 1358, 9000, 3, 83, 2149, 10, 355, 140, 794], [1, 4074, 491, 6, 9001, 28, 11, 158], [7, 287, 9, 36, 48, 22, 73, 4, 644, 9002, 114, 101, 346, 4, 271, 2, 9003, 3, 81, 2, 1, 3026, 2, 6457, 3, 282, 53, 34, 6458, 19, 339, 22, 43, 97, 608, 7, 450, 4, 36, 133, 1191, 88, 12, 133, 71, 914, 1, 759, 3027, 2, 9, 1445, 1359, 18, 760, 12, 4973, 6, 1, 421, 9004, 9005, 7, 214, 9, 36, 48, 22, 3449, 3028, 98, 124, 1192, 4, 1, 92, 9006, 6, 3450, 3, 7, 761, 870, 9, 36, 55, 111, 32]]\n"
     ]
    }
   ],
   "source": [
    "max_words = 50000 # Max size of the dictionary\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(author.values)\n",
    "sequences = tok.texts_to_sequences(author.values)\n",
    "print(sequences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of lists resulting from the tokenization. This will reduce the list\n",
    "# to one dimension, allowing us to apply the sliding window technique to predict the next word\n",
    "text = [item for sublist in sequences for item in sublist]\n",
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size in this corpus:  15713\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size in this corpus: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_len = 20\n",
    "pred_len = 1\n",
    "train_len = sentence_len - pred_len\n",
    "seq = []\n",
    "# Sliding window to generate test and train data\n",
    "for i in range(len(text)-sentence_len):\n",
    "    seq.append(text[i:i+sentence_len])\n",
    "# Reverse dictionary so as to decode tokenized sequences back to words and sentences\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "# dump(tok, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = []\n",
    "trainy = []\n",
    "for i in seq:\n",
    "    trainX.append(i[:train_len])\n",
    "    trainy.append(i[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture:\n",
    "1. Embedding layer\n",
    "    - Helps model understand 'meaning' of words by mapping them to representative vector space instead of semantic integers\n",
    "2. Stacked LSTM layers\n",
    "    - Stacked LSTMs add more depth than additional cells in a single LSTM layer (see: https://machinelearningmastery.com/stacked-long-short-term-memory-networks/)\n",
    "3. Dense (regression) layer\n",
    "4. Softmax activation for word probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size+1, 50, input_length=train_len),\n",
    "    LSTM(100, return_sequences=True),\n",
    "    LSTM(100),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 19, 50)            785700    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 19, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 15713)             1587013   \n",
      "=================================================================\n",
      "Total params: 2,523,613\n",
      "Trainable params: 2,523,613\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(np.asarray(trainX),\n",
    "         pd.get_dummies(np.asarray(trainy)),\n",
    "         epochs = 500,\n",
    "         batch_size = 10240,\n",
    "         callbacks = callbacks_list,\n",
    "         verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "201082/201082 [==============================] - 242s 1ms/sample - loss: 6.8433 - acc: 0.0803\n",
      "Epoch 2/100\n",
      "201082/201082 [==============================] - 246s 1ms/sample - loss: 6.3949 - acc: 0.1045\n",
      "Epoch 3/100\n",
      "201082/201082 [==============================] - 241s 1ms/sample - loss: 6.1931 - acc: 0.1163\n",
      "Epoch 4/100\n",
      "201082/201082 [==============================] - 242s 1ms/sample - loss: 6.0466 - acc: 0.1245\n",
      "Epoch 5/100\n",
      "201082/201082 [==============================] - 243s 1ms/sample - loss: 5.9306 - acc: 0.1302\n",
      "Epoch 6/100\n",
      "201082/201082 [==============================] - 243s 1ms/sample - loss: 5.8292 - acc: 0.1351\n",
      "Epoch 7/100\n",
      "201082/201082 [==============================] - 245s 1ms/sample - loss: 5.7382 - acc: 0.1388\n",
      "Epoch 8/100\n",
      "201082/201082 [==============================] - 255s 1ms/sample - loss: 5.6575 - acc: 0.1414\n",
      "Epoch 9/100\n",
      "201082/201082 [==============================] - 237s 1ms/sample - loss: 5.5819 - acc: 0.1444\n",
      "Epoch 10/100\n",
      "201082/201082 [==============================] - 230s 1ms/sample - loss: 5.5137 - acc: 0.1472\n",
      "Epoch 11/100\n",
      "201082/201082 [==============================] - 229s 1ms/sample - loss: 5.4504 - acc: 0.1496\n",
      "Epoch 12/100\n",
      "201082/201082 [==============================] - 229s 1ms/sample - loss: 5.3905 - acc: 0.1526\n",
      "Epoch 13/100\n",
      "201082/201082 [==============================] - 229s 1ms/sample - loss: 5.3327 - acc: 0.1550\n",
      "Epoch 14/100\n",
      "201082/201082 [==============================] - 229s 1ms/sample - loss: 5.2769 - acc: 0.1579\n",
      "Epoch 15/100\n",
      "201082/201082 [==============================] - 285s 1ms/sample - loss: 5.2229 - acc: 0.1598\n",
      "Epoch 16/100\n",
      "201082/201082 [==============================] - 245s 1ms/sample - loss: 5.1698 - acc: 0.1624\n",
      "Epoch 17/100\n",
      "201082/201082 [==============================] - 7524s 37ms/sample - loss: 5.1193 - acc: 0.1652\n",
      "Epoch 18/100\n",
      "201082/201082 [==============================] - 243s 1ms/sample - loss: 5.0714 - acc: 0.1676\n",
      "Epoch 19/100\n",
      "201082/201082 [==============================] - 239s 1ms/sample - loss: 5.0237 - acc: 0.1709\n",
      "Epoch 20/100\n",
      "201082/201082 [==============================] - 238s 1ms/sample - loss: 4.9789 - acc: 0.1739\n",
      "Epoch 21/100\n",
      "201082/201082 [==============================] - 233s 1ms/sample - loss: 4.9348 - acc: 0.1763\n",
      "Epoch 22/100\n",
      "201082/201082 [==============================] - 232s 1ms/sample - loss: 4.8947 - acc: 0.1792\n",
      "Epoch 23/100\n",
      "201082/201082 [==============================] - 233s 1ms/sample - loss: 4.8538 - acc: 0.1817\n",
      "Epoch 24/100\n",
      "201082/201082 [==============================] - 233s 1ms/sample - loss: 4.8165 - acc: 0.1840\n",
      "Epoch 25/100\n",
      "201082/201082 [==============================] - 233s 1ms/sample - loss: 4.7787 - acc: 0.1867\n",
      "Epoch 26/100\n",
      "201082/201082 [==============================] - 233s 1ms/sample - loss: 4.7435 - acc: 0.1894\n",
      "Epoch 27/100\n",
      "201082/201082 [==============================] - 234s 1ms/sample - loss: 4.7101 - acc: 0.1926\n",
      "Epoch 28/100\n",
      "201082/201082 [==============================] - 242s 1ms/sample - loss: 4.6737 - acc: 0.1952\n",
      "Epoch 29/100\n",
      "201082/201082 [==============================] - 255s 1ms/sample - loss: 4.6408 - acc: 0.1982\n",
      "Epoch 30/100\n",
      "201082/201082 [==============================] - 237s 1ms/sample - loss: 4.6073 - acc: 0.2016\n",
      "Epoch 31/100\n",
      "201082/201082 [==============================] - 228s 1ms/sample - loss: 4.5741 - acc: 0.2041\n",
      "Epoch 32/100\n",
      "201082/201082 [==============================] - 236s 1ms/sample - loss: 4.5398 - acc: 0.2069\n",
      "Epoch 33/100\n",
      "201082/201082 [==============================] - 243s 1ms/sample - loss: 4.5095 - acc: 0.2102\n",
      "Epoch 34/100\n",
      "173824/201082 [========================>.....] - ETA: 34s - loss: 4.4680 - acc: 0.2137"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(np.asarray(trainX), pd.get_dummies(np.asarray(trainy)), batch_size=128, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "model.save('model_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-ded180c2652e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.compile(optimizer=keras.optimizers.Adam(lr=0.001),\n\u001b[0m\u001b[1;32m      2\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m      4\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./weight_tr5.hdf5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(lr=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "filepath = \"./weight_tr5.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "history = model.fit(np.asarray(trainX),\n",
    "         pd.get_dummies(np.asarray(trainy)),\n",
    "         epochs = 500,\n",
    "         batch_size = 10240,\n",
    "         callbacks = callbacks_list,\n",
    "         verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
