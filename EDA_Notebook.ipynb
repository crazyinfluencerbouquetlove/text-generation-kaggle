{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# LSTM layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  7900\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "# Selecting Edgar Allen Poe as author style to emulate\n",
    "author = train_df[train_df['author'] == 'EAP'][\"text\"]\n",
    "print('Number of sentences: ',author.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize words in corpus using Keras Tokenizer.\n",
    "This assigns a unique integer to each word and then replaces all instances of that word with the integer. Tokenization is necessary for preparing data for embedding layer ( see below )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19, 2397, 80, 1001, 29, 31, 177, 2, 4073, 1, 1960, 2, 11, 3024, 15, 7, 110, 157, 41, 2146, 3, 481, 4, 1, 149, 2147, 7, 393, 74, 114, 101, 439, 2, 1, 162, 32, 913, 6453, 136, 1, 380], [6, 21, 142, 150, 10, 5, 551, 2148, 319, 28, 16, 15, 20, 8999, 128, 1, 3025, 2398, 30, 171, 2, 1797, 697, 20, 180, 2148, 6454, 12, 33, 188, 2, 1, 869, 243, 522, 1264], [1, 6455, 203, 14, 19, 149, 180, 6456, 6, 1, 1357, 2, 1358, 9000, 3, 83, 2149, 10, 355, 140, 794], [1, 4074, 491, 6, 9001, 28, 11, 158], [7, 287, 9, 36, 48, 22, 73, 4, 644, 9002, 114, 101, 346, 4, 271, 2, 9003, 3, 81, 2, 1, 3026, 2, 6457, 3, 282, 53, 34, 6458, 19, 339, 22, 43, 97, 608, 7, 450, 4, 36, 133, 1191, 88, 12, 133, 71, 914, 1, 759, 3027, 2, 9, 1445, 1359, 18, 760, 12, 4973, 6, 1, 421, 9004, 9005, 7, 214, 9, 36, 48, 22, 3449, 3028, 98, 124, 1192, 4, 1, 92, 9006, 6, 3450, 3, 7, 761, 870, 9, 36, 55, 111, 32]]\n"
     ]
    }
   ],
   "source": [
    "max_words = 50000 # Max size of the dictionary\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(author.values)\n",
    "sequences = tok.texts_to_sequences(author.values)\n",
    "print(sequences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of lists resulting from the tokenization. This will reduce the list\n",
    "# to one dimension, allowing us to apply the sliding window technique to predict the next word\n",
    "text = [item for sublist in sequences for item in sublist]\n",
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size in this corpus:  15713\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size in this corpus: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on 19 words to predict the 20th\n",
    "sentence_len = 20\n",
    "pred_len = 1\n",
    "train_len = sentence_len - pred_len\n",
    "seq = []\n",
    "# Sliding window to generate test and train data\n",
    "for i in range(len(text)-sentence_len):\n",
    "    seq.append(text[i:i+sentence_len])\n",
    "# Reverse dictionary so as to decode tokenized sequences back to words and sentences\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "# dump(tok, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = []\n",
    "trainy = []\n",
    "for i in seq:\n",
    "    trainX.append(i[:train_len])\n",
    "    trainy.append(i[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture:\n",
    "1. Embedding layer\n",
    "    - Helps model understand 'meaning' of words by mapping them to representative vector space instead of semantic integers\n",
    "2. Stacked LSTM layers\n",
    "    - Stacked LSTMs add more depth than additional cells in a single LSTM layer (see: https://machinelearningmastery.com/stacked-long-short-term-memory-networks/)\n",
    "3. Dense (regression) layer\n",
    "4. Dense layer with Softmax activation \n",
    "    - Outputs word probability across entire vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size+1, 50, input_length=train_len),\n",
    "    LSTM(100, return_sequences=True),\n",
    "    LSTM(100),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 19, 50)            785700    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 19, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 15713)             1587013   \n",
      "=================================================================\n",
      "Total params: 2,523,613\n",
      "Trainable params: 2,523,613\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(np.asarray(trainX),\n",
    "         pd.get_dummies(np.asarray(trainy)),\n",
    "         epochs = 500,\n",
    "         batch_size = 10240,\n",
    "         callbacks = callbacks_list,\n",
    "         verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "201082/201082 [==============================] - 242s 1ms/sample - loss: 6.8433 - acc: 0.0803\n",
      "Epoch 2/100\n",
      "201082/201082 [==============================] - 246s 1ms/sample - loss: 6.3949 - acc: 0.1045\n",
      "Epoch 3/100\n",
      "201082/201082 [==============================] - 241s 1ms/sample - loss: 6.1931 - acc: 0.1163\n",
      "Epoch 4/100\n",
      "201082/201082 [==============================] - 242s 1ms/sample - loss: 6.0466 - acc: 0.1245\n",
      "Epoch 5/100\n",
      "201082/201082 [==============================] - 243s 1ms/sample - loss: 5.9306 - acc: 0.1302\n",
      "Epoch 6/100\n",
      "201082/201082 [==============================] - 243s 1ms/sample - loss: 5.8292 - acc: 0.1351\n",
      "Epoch 7/100\n",
      "201082/201082 [==============================] - 245s 1ms/sample - loss: 5.7382 - acc: 0.1388\n",
      "Epoch 8/100\n",
      "201082/201082 [==============================] - 255s 1ms/sample - loss: 5.6575 - acc: 0.1414\n",
      "Epoch 9/100\n",
      "201082/201082 [==============================] - 237s 1ms/sample - loss: 5.5819 - acc: 0.1444\n",
      "Epoch 10/100\n",
      "201082/201082 [==============================] - 230s 1ms/sample - loss: 5.5137 - acc: 0.1472\n",
      "Epoch 11/100\n",
      "201082/201082 [==============================] - 229s 1ms/sample - loss: 5.4504 - acc: 0.1496\n",
      "Epoch 12/100\n",
      "201082/201082 [==============================] - 229s 1ms/sample - loss: 5.3905 - acc: 0.1526\n",
      "Epoch 13/100\n",
      "201082/201082 [==============================] - 229s 1ms/sample - loss: 5.3327 - acc: 0.1550\n",
      "Epoch 14/100\n",
      "201082/201082 [==============================] - 229s 1ms/sample - loss: 5.2769 - acc: 0.1579\n",
      "Epoch 15/100\n",
      "201082/201082 [==============================] - 285s 1ms/sample - loss: 5.2229 - acc: 0.1598\n",
      "Epoch 16/100\n",
      "201082/201082 [==============================] - 245s 1ms/sample - loss: 5.1698 - acc: 0.1624\n",
      "Epoch 17/100\n",
      "201082/201082 [==============================] - 7524s 37ms/sample - loss: 5.1193 - acc: 0.1652\n",
      "Epoch 18/100\n",
      "201082/201082 [==============================] - 243s 1ms/sample - loss: 5.0714 - acc: 0.1676\n",
      "Epoch 19/100\n",
      "201082/201082 [==============================] - 239s 1ms/sample - loss: 5.0237 - acc: 0.1709\n",
      "Epoch 20/100\n",
      "201082/201082 [==============================] - 238s 1ms/sample - loss: 4.9789 - acc: 0.1739\n",
      "Epoch 21/100\n",
      "201082/201082 [==============================] - 233s 1ms/sample - loss: 4.9348 - acc: 0.1763\n",
      "Epoch 22/100\n",
      "201082/201082 [==============================] - 232s 1ms/sample - loss: 4.8947 - acc: 0.1792\n",
      "Epoch 23/100\n",
      "201082/201082 [==============================] - 233s 1ms/sample - loss: 4.8538 - acc: 0.1817\n",
      "Epoch 24/100\n",
      "201082/201082 [==============================] - 233s 1ms/sample - loss: 4.8165 - acc: 0.1840\n",
      "Epoch 25/100\n",
      "201082/201082 [==============================] - 233s 1ms/sample - loss: 4.7787 - acc: 0.1867\n",
      "Epoch 26/100\n",
      "201082/201082 [==============================] - 233s 1ms/sample - loss: 4.7435 - acc: 0.1894\n",
      "Epoch 27/100\n",
      "201082/201082 [==============================] - 234s 1ms/sample - loss: 4.7101 - acc: 0.1926\n",
      "Epoch 28/100\n",
      "201082/201082 [==============================] - 242s 1ms/sample - loss: 4.6737 - acc: 0.1952\n",
      "Epoch 29/100\n",
      "201082/201082 [==============================] - 255s 1ms/sample - loss: 4.6408 - acc: 0.1982\n",
      "Epoch 30/100\n",
      "201082/201082 [==============================] - 237s 1ms/sample - loss: 4.6073 - acc: 0.2016\n",
      "Epoch 31/100\n",
      "201082/201082 [==============================] - 228s 1ms/sample - loss: 4.5741 - acc: 0.2041\n",
      "Epoch 32/100\n",
      "201082/201082 [==============================] - 236s 1ms/sample - loss: 4.5398 - acc: 0.2069\n",
      "Epoch 33/100\n",
      "201082/201082 [==============================] - 243s 1ms/sample - loss: 4.5095 - acc: 0.2102\n",
      "Epoch 34/100\n",
      "201082/201082 [==============================] - 252s 1ms/sample - loss: 4.4780 - acc: 0.2130\n",
      "Epoch 35/100\n",
      "201082/201082 [==============================] - 240s 1ms/sample - loss: 4.4459 - acc: 0.2158\n",
      "Epoch 36/100\n",
      "201082/201082 [==============================] - 253s 1ms/sample - loss: 4.4125 - acc: 0.2190\n",
      "Epoch 37/100\n",
      "201082/201082 [==============================] - 268s 1ms/sample - loss: 4.3833 - acc: 0.2223\n",
      "Epoch 38/100\n",
      "201082/201082 [==============================] - 262s 1ms/sample - loss: 4.3524 - acc: 0.2254\n",
      "Epoch 39/100\n",
      "201082/201082 [==============================] - 264s 1ms/sample - loss: 4.3237 - acc: 0.2281\n",
      "Epoch 40/100\n",
      "201082/201082 [==============================] - 272s 1ms/sample - loss: 4.2916 - acc: 0.2312\n",
      "Epoch 41/100\n",
      "201082/201082 [==============================] - 272s 1ms/sample - loss: 4.2611 - acc: 0.2352s - loss: 4\n",
      "Epoch 42/100\n",
      "201082/201082 [==============================] - 270s 1ms/sample - loss: 4.2337 - acc: 0.2385\n",
      "Epoch 43/100\n",
      "201082/201082 [==============================] - 262s 1ms/sample - loss: 4.2042 - acc: 0.2410\n",
      "Epoch 44/100\n",
      "201082/201082 [==============================] - 263s 1ms/sample - loss: 4.1774 - acc: 0.2449\n",
      "Epoch 45/100\n",
      "201082/201082 [==============================] - 265s 1ms/sample - loss: 4.1461 - acc: 0.2475\n",
      "Epoch 46/100\n",
      "201082/201082 [==============================] - 249s 1ms/sample - loss: 4.1229 - acc: 0.2508\n",
      "Epoch 47/100\n",
      "201082/201082 [==============================] - 246s 1ms/sample - loss: 4.0936 - acc: 0.2541\n",
      "Epoch 48/100\n",
      "201082/201082 [==============================] - 244s 1ms/sample - loss: 4.0663 - acc: 0.2578\n",
      "Epoch 49/100\n",
      "201082/201082 [==============================] - 242s 1ms/sample - loss: 4.0411 - acc: 0.2609\n",
      "Epoch 50/100\n",
      "201082/201082 [==============================] - 239s 1ms/sample - loss: 4.0149 - acc: 0.2639\n",
      "Epoch 51/100\n",
      "201082/201082 [==============================] - 239s 1ms/sample - loss: 3.9896 - acc: 0.2668\n",
      "Epoch 52/100\n",
      "201082/201082 [==============================] - 239s 1ms/sample - loss: 3.9618 - acc: 0.2705\n",
      "Epoch 53/100\n",
      "201082/201082 [==============================] - 240s 1ms/sample - loss: 3.9411 - acc: 0.2726\n",
      "Epoch 54/100\n",
      "201082/201082 [==============================] - 240s 1ms/sample - loss: 3.9169 - acc: 0.2755\n",
      "Epoch 55/100\n",
      "201082/201082 [==============================] - 238s 1ms/sample - loss: 3.8922 - acc: 0.2796\n",
      "Epoch 56/100\n",
      "201082/201082 [==============================] - 239s 1ms/sample - loss: 3.8672 - acc: 0.2814\n",
      "Epoch 57/100\n",
      "201082/201082 [==============================] - 237s 1ms/sample - loss: 3.8459 - acc: 0.2844\n",
      "Epoch 58/100\n",
      "201082/201082 [==============================] - 240s 1ms/sample - loss: 3.8215 - acc: 0.2883\n",
      "Epoch 59/100\n",
      "201082/201082 [==============================] - 246s 1ms/sample - loss: 3.8017 - acc: 0.2903\n",
      "Epoch 60/100\n",
      "201082/201082 [==============================] - 241s 1ms/sample - loss: 3.7767 - acc: 0.2940\n",
      "Epoch 61/100\n",
      "201082/201082 [==============================] - 240s 1ms/sample - loss: 3.7534 - acc: 0.2955\n",
      "Epoch 62/100\n",
      "201082/201082 [==============================] - 237s 1ms/sample - loss: 3.7324 - acc: 0.2992\n",
      "Epoch 63/100\n",
      "201082/201082 [==============================] - 236s 1ms/sample - loss: 3.7121 - acc: 0.3026\n",
      "Epoch 64/100\n",
      "201082/201082 [==============================] - 236s 1ms/sample - loss: 3.6913 - acc: 0.3056\n",
      "Epoch 65/100\n",
      "201082/201082 [==============================] - 230s 1ms/sample - loss: 3.6669 - acc: 0.3088\n",
      "Epoch 66/100\n",
      "201082/201082 [==============================] - 228s 1ms/sample - loss: 3.6476 - acc: 0.3111\n",
      "Epoch 67/100\n",
      "201082/201082 [==============================] - 236s 1ms/sample - loss: 3.6287 - acc: 0.3130\n",
      "Epoch 68/100\n",
      "201082/201082 [==============================] - 242s 1ms/sample - loss: 3.6097 - acc: 0.3168\n",
      "Epoch 69/100\n",
      "201082/201082 [==============================] - 242s 1ms/sample - loss: 3.5936 - acc: 0.3181\n",
      "Epoch 70/100\n",
      "201082/201082 [==============================] - 242s 1ms/sample - loss: 3.5696 - acc: 0.3212\n",
      "Epoch 71/100\n",
      "201082/201082 [==============================] - 242s 1ms/sample - loss: 3.5545 - acc: 0.3244\n",
      "Epoch 72/100\n",
      "201082/201082 [==============================] - 238s 1ms/sample - loss: 3.5364 - acc: 0.3263\n",
      "Epoch 73/100\n",
      "201082/201082 [==============================] - 233s 1ms/sample - loss: 3.5165 - acc: 0.3300\n",
      "Epoch 74/100\n",
      "201082/201082 [==============================] - 232s 1ms/sample - loss: 3.4983 - acc: 0.3313\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201082/201082 [==============================] - 231s 1ms/sample - loss: 3.4813 - acc: 0.3344\n",
      "Epoch 76/100\n",
      "201082/201082 [==============================] - 231s 1ms/sample - loss: 3.4604 - acc: 0.3378\n",
      "Epoch 77/100\n",
      "201082/201082 [==============================] - 231s 1ms/sample - loss: 3.4504 - acc: 0.3385\n",
      "Epoch 78/100\n",
      "201082/201082 [==============================] - 259s 1ms/sample - loss: 3.4325 - acc: 0.3404\n",
      "Epoch 79/100\n",
      "201082/201082 [==============================] - 242s 1ms/sample - loss: 3.4173 - acc: 0.3433\n",
      "Epoch 80/100\n",
      "201082/201082 [==============================] - 246s 1ms/sample - loss: 3.3962 - acc: 0.3461\n",
      "Epoch 81/100\n",
      "201082/201082 [==============================] - 257s 1ms/sample - loss: 3.3840 - acc: 0.3483\n",
      "Epoch 82/100\n",
      "201082/201082 [==============================] - 242s 1ms/sample - loss: 3.3629 - acc: 0.3515\n",
      "Epoch 83/100\n",
      "201082/201082 [==============================] - 255s 1ms/sample - loss: 3.3489 - acc: 0.3533\n",
      "Epoch 84/100\n",
      "201082/201082 [==============================] - 259s 1ms/sample - loss: 3.3341 - acc: 0.3558\n",
      "Epoch 85/100\n",
      "201082/201082 [==============================] - 259s 1ms/sample - loss: 3.3206 - acc: 0.3573\n",
      "Epoch 86/100\n",
      "201082/201082 [==============================] - 261s 1ms/sample - loss: 3.3004 - acc: 0.3603\n",
      "Epoch 87/100\n",
      "201082/201082 [==============================] - 246s 1ms/sample - loss: 3.2904 - acc: 0.3622\n",
      "Epoch 88/100\n",
      "201082/201082 [==============================] - 271s 1ms/sample - loss: 3.2745 - acc: 0.3643\n",
      "Epoch 89/100\n",
      "201082/201082 [==============================] - 248s 1ms/sample - loss: 3.2621 - acc: 0.3661\n",
      "Epoch 90/100\n",
      "201082/201082 [==============================] - 252s 1ms/sample - loss: 3.2443 - acc: 0.3688\n",
      "Epoch 91/100\n",
      "201082/201082 [==============================] - 258s 1ms/sample - loss: 3.2345 - acc: 0.3702\n",
      "Epoch 92/100\n",
      "201082/201082 [==============================] - 256s 1ms/sample - loss: 3.2210 - acc: 0.3727\n",
      "Epoch 93/100\n",
      "201082/201082 [==============================] - 258s 1ms/sample - loss: 3.2044 - acc: 0.3746\n",
      "Epoch 94/100\n",
      "201082/201082 [==============================] - 256s 1ms/sample - loss: 3.1964 - acc: 0.3758\n",
      "Epoch 95/100\n",
      "201082/201082 [==============================] - 256s 1ms/sample - loss: 3.1814 - acc: 0.3778\n",
      "Epoch 96/100\n",
      "201082/201082 [==============================] - 259s 1ms/sample - loss: 3.1695 - acc: 0.3798\n",
      "Epoch 97/100\n",
      "201082/201082 [==============================] - 253s 1ms/sample - loss: 3.1566 - acc: 0.3819\n",
      "Epoch 98/100\n",
      "201082/201082 [==============================] - 253s 1ms/sample - loss: 3.1442 - acc: 0.3839\n",
      "Epoch 99/100\n",
      "201082/201082 [==============================] - 246s 1ms/sample - loss: 3.1321 - acc: 0.3854\n",
      "Epoch 100/100\n",
      "201082/201082 [==============================] - 253s 1ms/sample - loss: 3.1184 - acc: 0.3877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a43927f98>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(np.asarray(trainX), pd.get_dummies(np.asarray(trainy)), batch_size=128, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "model.save('model_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=Adam(lr=0.001),\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# filepath = \"./weight_tr5.hdf5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "# callbacks_list = [checkpoint]\n",
    "# history = model.fit(np.asarray(trainX),\n",
    "#          pd.get_dummies(np.asarray(trainy)),\n",
    "#          epochs = 500,\n",
    "#          batch_size = 10240,\n",
    "#          callbacks = callbacks_list,\n",
    "#          verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(seq,max_len = 20):\n",
    "    sent = tokenizer.texts_to_sequences([seq])\n",
    "    #print(sent)\n",
    "    while len(sent[0]) < max_len:\n",
    "        sent2 = pad_sequences(sent[-19:],maxlen=19)\n",
    "        op = model.predict(np.asarray(sent2).reshape(1,-1))\n",
    "        sent[0].append(op.argmax()+1)\n",
    "    return \" \".join(map(lambda x : reverse_word_map[x],sent[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1='There was a big bear in the corner.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there was a big bear in the corner of the earth and the most practice is in the same moment when the whole town was topsy massa de la unus la bedloe deposes that in the whole of the most practice that had been in the world of being sure of the most indefinite sense of air is excessively early and sure but the most rarefied and ferry of be apprehend you to bishop in determining by the dust in a very degree the moon and the most practice of all who is not altogether the whole of the most'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(test1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
