{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edgar Allen Poe Text Generation Notebook\n",
    "**Jeremy Chow**\n",
    "\n",
    "7/15/2019\n",
    "\n",
    "Goal: Generate text in the style of Edgar Allen Poe, specifically emulating his writing style in the short story dataset from Kaggle \"Spooky Author Identification\" competition: https://www.kaggle.com/c/spooky-author-identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Here we need general dataframe manipulation libraries, then some TensorFlow and Keras libraries for the deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Standard Data Science Libraries\n",
    "import pickle\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "# Neural Net Preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# Neural Net Layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# Neural Net Training\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from pickle import load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences:  7900\n"
     ]
    }
   ],
   "source": [
    "# Import the data\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "# Selecting Edgar Allen Poe as author style to emulate\n",
    "author = train_df[train_df['author'] == 'EAP'][\"text\"]\n",
    "print('Number of training sentences: ',author.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Generally for NLP projects, to optimize the model's ability to gather meaning from the text, there would be removal of:\n",
    "- stop words such as _\"the\",\"a\",\"an\"_ \n",
    "- punctuation\n",
    "\n",
    "then tokenization (turning unique words into unique integers) of the text. However, because the goal here is to generate fluid and human-like speech, we want to preserve stop words. Instead we just use the Tokenizer method in the Keras library to perform the rest of the preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize words in corpus using Keras Tokenizer.\n",
    "This function does the following:\n",
    "1. Removes punctation\n",
    "2. Sets all text to lower case\n",
    "3. Splits the words up, then assigns a unique integer to each word\n",
    "4. Replaces all instances of that word with the integer.\n",
    "\n",
    "Tokenization is necessary for preparing data for embedding layer (see model architecture section below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19, 2397, 80, 1001, 29, 31, 177, 2, 4073, 1, 1960, 2, 11, 3024, 15, 7, 110, 157, 41, 2146, 3, 481, 4, 1, 149, 2147, 7, 393, 74, 114, 101, 439, 2, 1, 162, 32, 913, 6453, 136, 1, 380], [6, 21, 142, 150, 10, 5, 551, 2148, 319, 28, 16, 15, 20, 8999, 128, 1, 3025, 2398, 30, 171, 2, 1797, 697, 20, 180, 2148, 6454, 12, 33, 188, 2, 1, 869, 243, 522, 1264], [1, 6455, 203, 14, 19, 149, 180, 6456, 6, 1, 1357, 2, 1358, 9000, 3, 83, 2149, 10, 355, 140, 794], [1, 4074, 491, 6, 9001, 28, 11, 158], [7, 287, 9, 36, 48, 22, 73, 4, 644, 9002, 114, 101, 346, 4, 271, 2, 9003, 3, 81, 2, 1, 3026, 2, 6457, 3, 282, 53, 34, 6458, 19, 339, 22, 43, 97, 608, 7, 450, 4, 36, 133, 1191, 88, 12, 133, 71, 914, 1, 759, 3027, 2, 9, 1445, 1359, 18, 760, 12, 4973, 6, 1, 421, 9004, 9005, 7, 214, 9, 36, 48, 22, 3449, 3028, 98, 124, 1192, 4, 1, 92, 9006, 6, 3450, 3, 7, 761, 870, 9, 36, 55, 111, 32]]\n"
     ]
    }
   ],
   "source": [
    "max_words = 50000 # Max size of the dictionary\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(author.values)\n",
    "sequences = tokenizer.texts_to_sequences(author.values)\n",
    "print(sequences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of lists resulting from the tokenization. This will reduce the list\n",
    "# to one dimension, allowing us to apply the sliding window technique to predict the next word\n",
    "text = [item for sublist in sequences for item in sublist]\n",
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size in this corpus:  15713\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size in this corpus: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on 19 words to predict the 20th\n",
    "sentence_len = 20\n",
    "pred_len = 1\n",
    "train_len = sentence_len - pred_len\n",
    "seq = []\n",
    "# Sliding window to generate train data\n",
    "for i in range(len(text)-sentence_len):\n",
    "    seq.append(text[i:i+sentence_len])\n",
    "# Reverse dictionary to decode tokenized sequences back to words\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "# Save tokenizer\n",
    "# dump(tok, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each row in seq is a 20 word long window. We append he first 19 words as the input to predict the 20th word\n",
    "trainX = []\n",
    "trainy = []\n",
    "for i in seq:\n",
    "    trainX.append(i[:train_len])\n",
    "    trainy.append(i[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture:\n",
    "1. Embedding layer\n",
    "    - Helps model understand 'meaning' of words by mapping them to representative vector space instead of semantic integers\n",
    "2. Stacked LSTM layers\n",
    "    - Stacked LSTMs add more depth than additional cells in a single LSTM layer (see paper: https://arxiv.org/abs/1303.5778)\n",
    "    - The first LSTM layer must have `return sequences` flag set to True in order to pass sequence information to the second LSTM layer instead of just its end states\n",
    "3. Dense (regression) layer with ReLU activation\n",
    "4. Dense layer with Softmax activation \n",
    "    - Outputs word probability across entire vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size+1, 50, input_length=train_len),\n",
    "    LSTM(100, return_sequences=True),\n",
    "    LSTM(100),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 19, 50)            785700    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 19, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 15713)             1587013   \n",
      "=================================================================\n",
      "Total params: 2,523,613\n",
      "Trainable params: 2,523,613\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(np.asarray(trainX),\n",
    "         pd.get_dummies(np.asarray(trainy)),\n",
    "         epochs = 500,\n",
    "         batch_size = 10240,\n",
    "         callbacks = callbacks_list,\n",
    "         verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "201082/201082 [==============================] - 242s 1ms/sample - loss: 6.8433 - acc: 0.0803\n",
      "Epoch 2/100\n",
      "201082/201082 [==============================] - 246s 1ms/sample - loss: 6.3949 - acc: 0.1045\n",
      "Epoch 3/100\n",
      "201082/201082 [==============================] - 241s 1ms/sample - loss: 6.1931 - acc: 0.1163\n",
      "Epoch 4/100\n",
      "201082/201082 [==============================] - 242s 1ms/sample - loss: 6.0466 - acc: 0.1245\n",
      "Epoch 5/100\n",
      "201082/201082 [==============================] - 243s 1ms/sample - loss: 5.9306 - acc: 0.1302\n",
      "Epoch 6/100\n",
      "201082/201082 [==============================] - 243s 1ms/sample - loss: 5.8292 - acc: 0.1351\n",
      "Epoch 7/100\n",
      "201082/201082 [==============================] - 245s 1ms/sample - loss: 5.7382 - acc: 0.1388\n",
      "Epoch 8/100\n",
      "201082/201082 [==============================] - 255s 1ms/sample - loss: 5.6575 - acc: 0.1414\n",
      "Epoch 9/100\n",
      "201082/201082 [==============================] - 237s 1ms/sample - loss: 5.5819 - acc: 0.1444\n",
      "Epoch 10/100\n",
      "201082/201082 [==============================] - 230s 1ms/sample - loss: 5.5137 - acc: 0.1472\n",
      "Epoch 11/100\n",
      "201082/201082 [==============================] - 229s 1ms/sample - loss: 5.4504 - acc: 0.1496\n",
      "Epoch 12/100\n",
      "201082/201082 [==============================] - 229s 1ms/sample - loss: 5.3905 - acc: 0.1526\n",
      "Epoch 13/100\n",
      "201082/201082 [==============================] - 229s 1ms/sample - loss: 5.3327 - acc: 0.1550\n",
      "Epoch 14/100\n",
      "201082/201082 [==============================] - 229s 1ms/sample - loss: 5.2769 - acc: 0.1579\n",
      "Epoch 15/100\n",
      "201082/201082 [==============================] - 285s 1ms/sample - loss: 5.2229 - acc: 0.1598\n",
      "Epoch 16/100\n",
      "201082/201082 [==============================] - 245s 1ms/sample - loss: 5.1698 - acc: 0.1624\n",
      "Epoch 17/100\n",
      "201082/201082 [==============================] - 7524s 37ms/sample - loss: 5.1193 - acc: 0.1652\n",
      "Epoch 18/100\n",
      "201082/201082 [==============================] - 243s 1ms/sample - loss: 5.0714 - acc: 0.1676\n",
      "Epoch 19/100\n",
      "201082/201082 [==============================] - 239s 1ms/sample - loss: 5.0237 - acc: 0.1709\n",
      "Epoch 20/100\n",
      "201082/201082 [==============================] - 238s 1ms/sample - loss: 4.9789 - acc: 0.1739\n",
      "Epoch 21/100\n",
      "201082/201082 [==============================] - 233s 1ms/sample - loss: 4.9348 - acc: 0.1763\n",
      "Epoch 22/100\n",
      "201082/201082 [==============================] - 232s 1ms/sample - loss: 4.8947 - acc: 0.1792\n",
      "Epoch 23/100\n",
      "201082/201082 [==============================] - 233s 1ms/sample - loss: 4.8538 - acc: 0.1817\n",
      "Epoch 24/100\n",
      "201082/201082 [==============================] - 233s 1ms/sample - loss: 4.8165 - acc: 0.1840\n",
      "Epoch 25/100\n",
      "201082/201082 [==============================] - 233s 1ms/sample - loss: 4.7787 - acc: 0.1867\n",
      "Epoch 26/100\n",
      "201082/201082 [==============================] - 233s 1ms/sample - loss: 4.7435 - acc: 0.1894\n",
      "Epoch 27/100\n",
      "201082/201082 [==============================] - 234s 1ms/sample - loss: 4.7101 - acc: 0.1926\n",
      "Epoch 28/100\n",
      "201082/201082 [==============================] - 242s 1ms/sample - loss: 4.6737 - acc: 0.1952\n",
      "Epoch 29/100\n",
      "201082/201082 [==============================] - 255s 1ms/sample - loss: 4.6408 - acc: 0.1982\n",
      "Epoch 30/100\n",
      "201082/201082 [==============================] - 237s 1ms/sample - loss: 4.6073 - acc: 0.2016\n",
      "Epoch 31/100\n",
      "201082/201082 [==============================] - 228s 1ms/sample - loss: 4.5741 - acc: 0.2041\n",
      "Epoch 32/100\n",
      "201082/201082 [==============================] - 236s 1ms/sample - loss: 4.5398 - acc: 0.2069\n",
      "Epoch 33/100\n",
      "201082/201082 [==============================] - 243s 1ms/sample - loss: 4.5095 - acc: 0.2102\n",
      "Epoch 34/100\n",
      "201082/201082 [==============================] - 252s 1ms/sample - loss: 4.4780 - acc: 0.2130\n",
      "Epoch 35/100\n",
      "201082/201082 [==============================] - 240s 1ms/sample - loss: 4.4459 - acc: 0.2158\n",
      "Epoch 36/100\n",
      "201082/201082 [==============================] - 253s 1ms/sample - loss: 4.4125 - acc: 0.2190\n",
      "Epoch 37/100\n",
      "201082/201082 [==============================] - 268s 1ms/sample - loss: 4.3833 - acc: 0.2223\n",
      "Epoch 38/100\n",
      "201082/201082 [==============================] - 262s 1ms/sample - loss: 4.3524 - acc: 0.2254\n",
      "Epoch 39/100\n",
      "201082/201082 [==============================] - 264s 1ms/sample - loss: 4.3237 - acc: 0.2281\n",
      "Epoch 40/100\n",
      "201082/201082 [==============================] - 272s 1ms/sample - loss: 4.2916 - acc: 0.2312\n",
      "Epoch 41/100\n",
      "201082/201082 [==============================] - 272s 1ms/sample - loss: 4.2611 - acc: 0.2352s - loss: 4\n",
      "Epoch 42/100\n",
      "201082/201082 [==============================] - 270s 1ms/sample - loss: 4.2337 - acc: 0.2385\n",
      "Epoch 43/100\n",
      "201082/201082 [==============================] - 262s 1ms/sample - loss: 4.2042 - acc: 0.2410\n",
      "Epoch 44/100\n",
      "201082/201082 [==============================] - 263s 1ms/sample - loss: 4.1774 - acc: 0.2449\n",
      "Epoch 45/100\n",
      "201082/201082 [==============================] - 265s 1ms/sample - loss: 4.1461 - acc: 0.2475\n",
      "Epoch 46/100\n",
      "201082/201082 [==============================] - 249s 1ms/sample - loss: 4.1229 - acc: 0.2508\n",
      "Epoch 47/100\n",
      "201082/201082 [==============================] - 246s 1ms/sample - loss: 4.0936 - acc: 0.2541\n",
      "Epoch 48/100\n",
      "201082/201082 [==============================] - 244s 1ms/sample - loss: 4.0663 - acc: 0.2578\n",
      "Epoch 49/100\n",
      "201082/201082 [==============================] - 242s 1ms/sample - loss: 4.0411 - acc: 0.2609\n",
      "Epoch 50/100\n",
      "201082/201082 [==============================] - 239s 1ms/sample - loss: 4.0149 - acc: 0.2639\n",
      "Epoch 51/100\n",
      "201082/201082 [==============================] - 239s 1ms/sample - loss: 3.9896 - acc: 0.2668\n",
      "Epoch 52/100\n",
      "201082/201082 [==============================] - 239s 1ms/sample - loss: 3.9618 - acc: 0.2705\n",
      "Epoch 53/100\n",
      "201082/201082 [==============================] - 240s 1ms/sample - loss: 3.9411 - acc: 0.2726\n",
      "Epoch 54/100\n",
      "201082/201082 [==============================] - 240s 1ms/sample - loss: 3.9169 - acc: 0.2755\n",
      "Epoch 55/100\n",
      "201082/201082 [==============================] - 238s 1ms/sample - loss: 3.8922 - acc: 0.2796\n",
      "Epoch 56/100\n",
      "201082/201082 [==============================] - 239s 1ms/sample - loss: 3.8672 - acc: 0.2814\n",
      "Epoch 57/100\n",
      "201082/201082 [==============================] - 237s 1ms/sample - loss: 3.8459 - acc: 0.2844\n",
      "Epoch 58/100\n",
      "201082/201082 [==============================] - 240s 1ms/sample - loss: 3.8215 - acc: 0.2883\n",
      "Epoch 59/100\n",
      "201082/201082 [==============================] - 246s 1ms/sample - loss: 3.8017 - acc: 0.2903\n",
      "Epoch 60/100\n",
      "201082/201082 [==============================] - 241s 1ms/sample - loss: 3.7767 - acc: 0.2940\n",
      "Epoch 61/100\n",
      "201082/201082 [==============================] - 240s 1ms/sample - loss: 3.7534 - acc: 0.2955\n",
      "Epoch 62/100\n",
      "201082/201082 [==============================] - 237s 1ms/sample - loss: 3.7324 - acc: 0.2992\n",
      "Epoch 63/100\n",
      "201082/201082 [==============================] - 236s 1ms/sample - loss: 3.7121 - acc: 0.3026\n",
      "Epoch 64/100\n",
      "201082/201082 [==============================] - 236s 1ms/sample - loss: 3.6913 - acc: 0.3056\n",
      "Epoch 65/100\n",
      "201082/201082 [==============================] - 230s 1ms/sample - loss: 3.6669 - acc: 0.3088\n",
      "Epoch 66/100\n",
      "201082/201082 [==============================] - 228s 1ms/sample - loss: 3.6476 - acc: 0.3111\n",
      "Epoch 67/100\n",
      "201082/201082 [==============================] - 236s 1ms/sample - loss: 3.6287 - acc: 0.3130\n",
      "Epoch 68/100\n",
      "201082/201082 [==============================] - 242s 1ms/sample - loss: 3.6097 - acc: 0.3168\n",
      "Epoch 69/100\n",
      "201082/201082 [==============================] - 242s 1ms/sample - loss: 3.5936 - acc: 0.3181\n",
      "Epoch 70/100\n",
      "201082/201082 [==============================] - 242s 1ms/sample - loss: 3.5696 - acc: 0.3212\n",
      "Epoch 71/100\n",
      "201082/201082 [==============================] - 242s 1ms/sample - loss: 3.5545 - acc: 0.3244\n",
      "Epoch 72/100\n",
      "201082/201082 [==============================] - 238s 1ms/sample - loss: 3.5364 - acc: 0.3263\n",
      "Epoch 73/100\n",
      "201082/201082 [==============================] - 233s 1ms/sample - loss: 3.5165 - acc: 0.3300\n",
      "Epoch 74/100\n",
      "201082/201082 [==============================] - 232s 1ms/sample - loss: 3.4983 - acc: 0.3313\n",
      "Epoch 75/100\n",
      "201082/201082 [==============================] - 231s 1ms/sample - loss: 3.4813 - acc: 0.3344\n",
      "Epoch 76/100\n",
      "201082/201082 [==============================] - 231s 1ms/sample - loss: 3.4604 - acc: 0.3378\n",
      "Epoch 77/100\n",
      "201082/201082 [==============================] - 231s 1ms/sample - loss: 3.4504 - acc: 0.3385\n",
      "Epoch 78/100\n",
      "201082/201082 [==============================] - 259s 1ms/sample - loss: 3.4325 - acc: 0.3404\n",
      "Epoch 79/100\n",
      "201082/201082 [==============================] - 242s 1ms/sample - loss: 3.4173 - acc: 0.3433\n",
      "Epoch 80/100\n",
      "201082/201082 [==============================] - 246s 1ms/sample - loss: 3.3962 - acc: 0.3461\n",
      "Epoch 81/100\n",
      "201082/201082 [==============================] - 257s 1ms/sample - loss: 3.3840 - acc: 0.3483\n",
      "Epoch 82/100\n",
      "201082/201082 [==============================] - 242s 1ms/sample - loss: 3.3629 - acc: 0.3515\n",
      "Epoch 83/100\n",
      "201082/201082 [==============================] - 255s 1ms/sample - loss: 3.3489 - acc: 0.3533\n",
      "Epoch 84/100\n",
      "201082/201082 [==============================] - 259s 1ms/sample - loss: 3.3341 - acc: 0.3558\n",
      "Epoch 85/100\n",
      "201082/201082 [==============================] - 259s 1ms/sample - loss: 3.3206 - acc: 0.3573\n",
      "Epoch 86/100\n",
      "201082/201082 [==============================] - 261s 1ms/sample - loss: 3.3004 - acc: 0.3603\n",
      "Epoch 87/100\n",
      "201082/201082 [==============================] - 246s 1ms/sample - loss: 3.2904 - acc: 0.3622\n",
      "Epoch 88/100\n",
      "201082/201082 [==============================] - 271s 1ms/sample - loss: 3.2745 - acc: 0.3643\n",
      "Epoch 89/100\n",
      "201082/201082 [==============================] - 248s 1ms/sample - loss: 3.2621 - acc: 0.3661\n",
      "Epoch 90/100\n",
      "201082/201082 [==============================] - 252s 1ms/sample - loss: 3.2443 - acc: 0.3688\n",
      "Epoch 91/100\n",
      "201082/201082 [==============================] - 258s 1ms/sample - loss: 3.2345 - acc: 0.3702\n",
      "Epoch 92/100\n",
      "201082/201082 [==============================] - 256s 1ms/sample - loss: 3.2210 - acc: 0.3727\n",
      "Epoch 93/100\n",
      "201082/201082 [==============================] - 258s 1ms/sample - loss: 3.2044 - acc: 0.3746\n",
      "Epoch 94/100\n",
      "201082/201082 [==============================] - 256s 1ms/sample - loss: 3.1964 - acc: 0.3758\n",
      "Epoch 95/100\n",
      "201082/201082 [==============================] - 256s 1ms/sample - loss: 3.1814 - acc: 0.3778\n",
      "Epoch 96/100\n",
      "201082/201082 [==============================] - 259s 1ms/sample - loss: 3.1695 - acc: 0.3798\n",
      "Epoch 97/100\n",
      "201082/201082 [==============================] - 253s 1ms/sample - loss: 3.1566 - acc: 0.3819\n",
      "Epoch 98/100\n",
      "201082/201082 [==============================] - 253s 1ms/sample - loss: 3.1442 - acc: 0.3839\n",
      "Epoch 99/100\n",
      "201082/201082 [==============================] - 246s 1ms/sample - loss: 3.1321 - acc: 0.3854\n",
      "Epoch 100/100\n",
      "201082/201082 [==============================] - 253s 1ms/sample - loss: 3.1184 - acc: 0.3877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a43927f98>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(np.asarray(trainX), pd.get_dummies(np.asarray(trainy)), batch_size=128, epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1 was trained for 100 epochs and ended with an accuracy of .3877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "model.save('model_weights.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Version 2\n",
    "This model is similar to model 1, but we add a dropout layer to prevent overfitting. The dropout layer randomly turns off a proportion of neurons fed into it from the previous layer, forcing the model to come up with more robust features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model_2 = Sequential([\n",
    "    Embedding(vocab_size+1, 50, input_length=train_len),\n",
    "    LSTM(100, return_sequences=True),\n",
    "    LSTM(100),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 19, 50)            785700    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 19, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 15713)             1587013   \n",
      "=================================================================\n",
      "Total params: 2,523,613\n",
      "Trainable params: 2,523,613\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping allows model to stop training if improvement stops.\n",
    "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "# model_2.compile(optimizer='adam',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# filepath = \"./model_2_weights.hdf5\"\n",
    "# # Model checkpointing allows us to preserve progress during training if training is interrupted\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "# callbacks_list = [checkpoint]\n",
    "# history = model_2.fit(np.asarray(trainX),\n",
    "#          pd.get_dummies(np.asarray(trainy)),\n",
    "#          epochs = 300,\n",
    "#          batch_size = 128,\n",
    "#          callbacks = callbacks_list,\n",
    "#          verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2 was trained for 300 epochs but only had .3138 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.load_weights('../models/model_2_weights_colab.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I moved model training to Google Colabs for the GPU boost to speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2 had an additional dropout layer, but the accuracy took a 30% hit.\n",
    "\n",
    "For model 3, we'll try removing the dropout layer and up the number of neurons across all layers by 50%. \n",
    "\n",
    "As expected, this resulted in a higher accuracy on the training set of about 40%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model_3 = Sequential([\n",
    "    Embedding(vocab_size+1, 50, input_length=train_len),\n",
    "    LSTM(150, return_sequences=True),\n",
    "    LSTM(150),\n",
    "    Dense(150, activation='relu'),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3 was trained for 300 epochs and reached .63 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaded after being trained remotely using Google Colab\n",
    "model_3.load_weights('../models/model_3_weights_colab.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the generation models\n",
    "If this were any other type of project then a good metric to quantify the model's success would be to do a **train-test split to identify the testing accuracy score** using the models to predict data it was not trained on and had never seen before. \n",
    "\n",
    "However, the goal of text generation isn't quite to maximize accuracy, because that would amount to the model regurgitating quotes and would be overfitting. Instead we'll compare the model outputs to the same input strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen(model,seq,max_len = 20):\n",
    "    ''' Generates a sequence given a string seq using specified model until the total sequence length\n",
    "    reaches max_len'''\n",
    "    # Tokenize the input string\n",
    "    tokenized_sent = tokenizer.texts_to_sequences([seq])\n",
    "    max_len = max_len+len(tokenized_sent[0])\n",
    "    # If sentence is not as long as the desired sentence length, we need to 'pad sequence' so that\n",
    "    # the array input shape is correct going into our LSTM. the `pad_sequences` function adds \n",
    "    # zeroes to the left side of our sequence until it becomes 19 long, the number of input features.\n",
    "    while len(tokenized_sent[0]) < max_len:\n",
    "        padded_sentence = pad_sequences(tokenized_sent[-19:],maxlen=19)\n",
    "        op = model.predict(np.asarray(padded_sentence).reshape(1,-1))\n",
    "        tokenized_sent[0].append(op.argmax()+1)\n",
    "        \n",
    "    return \" \".join(map(lambda x : reverse_word_map[x],tokenized_sent[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_models(test_string,sequence_length= 50, model_list = model_list):\n",
    "    '''Generates output given input test_string up to sequence_length'''\n",
    "    print('Input String: ', test_string)\n",
    "    for counter,model in enumerate(model_list):\n",
    "        print(\"Model \", counter+1, \":\")\n",
    "        print(gen(model,test_string,sequence_length))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [model,model_2,model_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input String:  This process however afforded me\n",
      "Model  1 :\n",
      "this process however afforded me bearing pernicious pernicious pernicious mattock mattock thousands thousands thousands disdain\n",
      "Model  2 :\n",
      "this process however afforded me most well echoed one who had a very idea which\n",
      "Model  3 :\n",
      "this process however afforded me good difficulty whatever long vaulting are no more subject to\n"
     ]
    }
   ],
   "source": [
    "test_models('This process however afforded me', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input String:  what avails the vigilance against the Destiny of man?\n",
      "Model  1 :\n",
      "what avails the vigilance against the destiny of man d'indaginé hippocratian hippocratian hippocratian miseries disclosed disclosed caligula worthy worthy engulfed shaved shaved shaved deviate deviate deviate hoggishly hoggishly hoggishly odd odd odd odd odd odd odd recitative recitative recitative succumbed evinced evinced evinced evinced definition definition definition definition definition definition caligula caligula caligula caligula fillagree fillagree fillagree fillagree fillagree\n",
      "Model  2 :\n",
      "what avails the vigilance against the destiny of man varnished with the most officer diameter to indebted at the same hour that looked up all ah we can say that the most officer which i had observed that so around and the whole hand of the most absurd lilies within the same time the most oil of shadows of\n",
      "Model  3 :\n",
      "what avails the vigilance against the destiny of man to their nationality that von jung them in unquiet thirst at this moment before evermore the most part of its grateful delight in its results to this interval out of a man decidedly let me see nothing that of light and dark oh a certain mark of surf and seeming\n"
     ]
    }
   ],
   "source": [
    "test_models(author.iloc[3709])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input String:  By these means for they were ignorant men I found little difficulty in gaining them over to my purpose.\n",
      "Model  1 :\n",
      "by these means for they were ignorant men i found little difficulty in gaining them over to my purpose worthy worthy worthy deviate perpetual perpetual perpetual velocities broadway hoggishly hoggishly pernicious persuaded glanced amazing generalities generalities pocket carrying carrying carrying pernicious jonas sign tavern tavern hideous hideous hideous fxwl type type torches torches indeterminate indeterminate 'und 'und 'und resume resume calf 'und calf calf 'und thus thus buries buries\n",
      "Model  2 :\n",
      "by these means for they were ignorant men i found little difficulty in gaining them over to my purpose in fact a very very appearance i have poh alluded that g he carries her idea and then forests in fortune however i am not a very angel book the same agitation recess i was made was more and more properly had been interested and as unreal what has a\n",
      "Model  3 :\n",
      "by these means for they were ignorant men i found little difficulty in gaining them over to my purpose he retained a single air maintained to be more false voluptuousness theatrical occasional excitement was not impeded by the gay and the seed singularly cutter in moss yet therefore i have already visited the audience were that the rays of the poetic sentiment has run full imperfect point for something\n"
     ]
    }
   ],
   "source": [
    "test_models(author.iloc[58],50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input String:  In the meantime it was folly to grieve, or to think.\n",
      "Model  1 :\n",
      "in the meantime it was folly to grieve or to think deviate deviate deviate hoggishly hoggishly odd odd odd odd odd odd recitative recitative recitative succumbed succumbed evinced evinced aorta glassy exaggeration exaggeration exaggeration year greatest greatest greatest hers hers phosphoric phosphoric phosphoric phosphoric dogmaticians dogmaticians landlady duty recorded recorded discontinuance stir recorded officer officer relieves flayed gleaming gleaming gleaming gleaming\n",
      "Model  2 :\n",
      "in the meantime it was folly to grieve or to think that the whole hand was intolerably folded in bob handkerchiefs which expired with a fiend i made in the most officer proprietor as so was at the whole of the oil of bob so a very capital height that caused upon the most oil of water had been obtained to\n",
      "Model  3 :\n",
      "in the meantime it was folly to grieve or to think the passions of the son of the automaton was in the first instance let us be impatient his right and incoherent tones towards the surrounding wealth in which conversing into the inner part below to see it stricken and down down like the ribs as well as i am aroused\n"
     ]
    }
   ],
   "source": [
    "test_models(author.iloc[70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input String:  First of all I dismembered the corpse.\n",
      "Model  1 :\n",
      "first of all i dismembered the corpse eder eder hippocratian hippocratian ladies ladies ladies amazing amazing amazing amazing pernicious pernicious pernicious pernicious fillagree fillagree fillagree fillagree or or mattock mattock strived strived pernicious pernicious pernicious canoe canoe canoe diverted discourage discourage discourage discourage humoredly humoredly humoredly humoredly humoredly humoredly proceedings infirmity infirmity infirmity characters characters dodona study\n",
      "Model  2 :\n",
      "first of all i dismembered the corpse telescope delighted and accordingly the whole idea was intended of course and the most oil of mankind nor the whole flight of us at all events had been mistaken the most idea of a very very old lady had been more so much a thousand channels which not at west\n",
      "Model  3 :\n",
      "first of all i dismembered the corpse which is profound since in the first place he appeared at first so suddenly as any matter no answer was impossible to find my sake he now happened to be sure it was he suspected or caution or gods and some voice held forth upon view the conditions was a\n"
     ]
    }
   ],
   "source": [
    "test_models(author.iloc[7800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input String:  Here, however, are Moissart, Voissart, Croissart, and Froissart, all in the direct line of descent.\n",
      "Model  1 :\n",
      "here however are moissart voissart croissart and froissart all in the direct line of descent 'oppodeldoc d'indaginé d'indaginé emissaries emissaries 'oppodeldoc 'oppodeldoc emissaries fell suspecting suspecting suspecting completed completed completed entreaties entreaties fossillus fossillus fossillus suspended suspended officer' smells concert payment payment ellison's choctaws concert thrusting juniper pure exaggeration snatches snatches concert security bearing happened relieves emerging fiftieth recorded supplying discontinuance butcheries gleaming unhurried unhurried\n",
      "Model  2 :\n",
      "here however are moissart voissart croissart and froissart all in the direct line of descent the most officer which the most oil of water had no the very natural man had been more more than the most officer which at the same time a little fellow in fact was well as that the idea was very early a little important body more certainly had that\n",
      "Model  3 :\n",
      "here however are moissart voissart croissart and froissart all in the direct line of descent there is no trouble in regarding it i found it impossible to do with a moment than weeks it is now i dared not to be sure it is it a wide matter an excellent imagination is in the french horn i observed to himself done by the limits of\n"
     ]
    }
   ],
   "source": [
    "test_models(author.iloc[7120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input String:  \"There are two windows in the chamber.\n",
      "Model  1 :\n",
      "there are two windows in the chamber 'found 'found 'found relieves relieves recitative disclosed disclosed disclosed engulfed engulfed engulfed hers hers hers shakespeare shakespeare 'prince thousands thousands thousands thousands involving sworn ellipsoid ellipsoid disdain imbued imbued pocket pocket museums museums museums memory memory senty senty senty senty senty confused confused confused confused holies holies holies riveted riveted\n",
      "Model  2 :\n",
      "there are two windows in the chamber had been more properly in fact that the idea was most much and the most most oil of water had been a very very old lady have been more than more more than the most oil of water the most oil of water the very surface of sighs wherein the\n",
      "Model  3 :\n",
      "there are two windows in the chamber of caravaggio of the amenity of albano to the renewed cares of the variable particulars of the monk i had enough although i am well the whole head spirits without sepulchrum magnetics n the chin was sometimes doing with the necessary courage everybody drove off like once as a single\n"
     ]
    }
   ],
   "source": [
    "test_models(author.iloc[5121])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible Improvements\n",
    "1. The model vocabulary is only based on the corpus vocabulary, so it omits any words in the input string that it does not recognize.\n",
    "2. To make a more robust model with a wider vocabulary, a pretrained embedding model like Glove for Fastext could be implemented"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
